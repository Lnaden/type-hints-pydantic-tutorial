{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25895379",
   "metadata": {},
   "source": [
    "(ch05)=\n",
    "# Validating Data Beyond Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe112157",
   "metadata": {},
   "source": [
    "```{admonition} Starting File: <code>04_pydantic_molecule.py</code>\n",
    ":class: important\n",
    "This chapter will start from the <code>04_pydantic_molecule.py</code> and end on the <code>05_valid_pydantic_molecule.py</code>.\n",
    "```\n",
    "\n",
    "Data validation goes far beyond just type. *Pydantic* has provided the basic tools for doing data validation on data types, but it also provides the tools for writing custom validators to check so much more.\n",
    "\n",
    "We'll be covering the *pydantic* `validator` decorator and applying that to our data to check structure and scientific rigor. We'll also cover how to validate types not native to Python, such as NumPy arrays.\n",
    "\n",
    "```{admonition} Check Out Pydantic\n",
    ":class: note\n",
    "We will not be covering all the capabilities of *pydantic* here, and we highly encourage you to visit [the pydantic docs](https://pydantic-docs.helpmanual.io/) to learn about all the powerful and easy-to-execute things *pydantic* can do.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Compatibility with Python 3.8 and below\n",
    ":class: note\n",
    "If you have Python 3.8 or below, you will need to import container type objects such as `List`, `Tuple`, `Dict`, etc. from the `typing` library instead of their native types of `list`, `tuple`, `dict`, etc. This chapter will assume Python 3.9 or greater, however, both approaches will work in >=Python 3.9 and have 1:1 replacements of the same name.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd72c9",
   "metadata": {},
   "source": [
    "## Pydantic's Validator Decorator\n",
    "\n",
    "Let's start by looking at the state of our code prior to extending the validators. As usual, let's also define our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35ea97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "\n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d440290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_data = {  # Good data\n",
    "    \"coordinates\": [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "    \"symbols\": [\"H\", \"H\", \"O\"], \n",
    "    \"charge\": 0.0, \n",
    "    \"name\": \"water\"\n",
    "}\n",
    "\n",
    "bad_name = {\"name\": 789}  # Name is not str\n",
    "bad_charge = {\"charge\": [1, 0.0]}  # Charge is not int or float\n",
    "noniter_symbols = {\"symbols\": 1234567890}  # Symbols is an int\n",
    "nonlist_symbols = {\"symbols\": '[\"H\", \"H\", \"O\"]'}  # Symbols is a string (notably is a string-ified list)\n",
    "tuple_symbols = {\"symbols\": (\"H\", \"H\", \"O\")}  # Symbols as a tuple?\n",
    "bad_coords = {\"coordinates\": [\"1\", \"2\", \"3\"]}  # Coords is a single list of string\n",
    "inner_coords_not3d = {\"coordinates\": [[1, 2, 3], [4, 5]]}\n",
    "bad_symbols_and_cords = {\"symbols\": [\"H\", \"H\", \"O\"],\n",
    "                         \"coordinates\": [[1, 1, 1], [2.0, 2.0, 2.0]]\n",
    "                        }  # Coordinates top-level list is not the same length as symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a19b",
   "metadata": {},
   "source": [
    "You may notice we have extended our \"Good Data\" here to have `coordinates` actually define the `Nx3` structure where `N = len(symbols)`. This is important for what we plan to validate.\n",
    "\n",
    "*pydantic* allows you to write custom validators, in addition to the type validators which run automatically for a type annotation. This `validator` is pulled from the `pydantic` module just like `BaseModel`, and is used to decorate a *class* function you write. Let's look at the most basic `validator` we can write and assign it to `coordinates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229837c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        return coords\n",
    "\n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa0030",
   "metadata": {},
   "source": [
    "Here we have defined an additional validator which does nothing, but has the basic structure we can look at. For convenience and reference, I've broken the aspects of the `validator` into a list.\n",
    "\n",
    "* The `validator` decorator takes as arguments the *exact* name of the attributes you are validating against as a string. In this case `coordinates`. You could provide multiple string args of each attribute you want to run through the validator if you want to reuse it.\n",
    "* The function name can be whatever you want it to be. We've called it `ensure_coordinates_is_3D` to be meaningful if anyone ever wants to come back and see what this should be doing.\n",
    "* The function itself is a *class function*. Similar to what happens when you use the `@classmethod` decorator from native Python, this validator is intended to be called on the non-instanced class. The formal nomenclature for the first variable here is therefore `cls` and not `self`. Your IDE may complain about this, but it should be `cls`. \n",
    "* The first argument of the function can be whatever string name you want EXCLUDING the following list: `values`, `config`, and `field` (reasons discussed later in this chapter).\n",
    "* The return MUST be the validated data to be fed into the attribute. We've done nothing to our variable `coords`, so we simply return it. If you fail to have a `return` statement with something, it will return `None` and that will be considered valid.\n",
    "* `validator` runs *after* type validation, unless specified (see later in this chapter).\n",
    "\n",
    "That may seem like lots of rules, but most of them are boilerplate and intuitive. Let's apply these items to our validator. We want to make sure the inner lists of `coordinates` are 3D, or length 3. We don't have to worry about type checking (that was done before any custom `validator` was run), so we can just do an iteration of the top list and make sure. Let's apply that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a2e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8cb3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  Inner coordinates must be 3D, got [4.0, 5.0] of length 2 (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m good_water \u001b[38;5;241m=\u001b[39m Molecule(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data)\n\u001b[1;32m      2\u001b[0m mangled \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_coords_not3d}\n\u001b[0;32m----> 3\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  Inner coordinates must be 3D, got [4.0, 5.0] of length 2 (type=value_error)"
     ]
    }
   ],
   "source": [
    "good_water = Molecule(**mol_data)\n",
    "mangled = {**mol_data, **inner_coords_not3d}\n",
    "water = Molecule(**mangled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5196d1e",
   "metadata": {},
   "source": [
    "Here we have checked the good data still works, and checked that the mangled data raised an error. It's important to note the error raised by the function can be any type of error, but what came out in the error report was a `ValidationError`. We can also see the error message is what we put as the error string and `type` of error is of the type we raised. This is why it's very important to have meaningful error strings when your custom validator fails.\n",
    "\n",
    "With all that said, our validator function really does look like any other function we may call to do a quick check of data, and then some special addons to make it work with *pydantic*. There is no practical limit to the number of `validator`s you have in a given class, so validate to your heart's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39d716",
   "metadata": {},
   "source": [
    "```{admonition} Python Assignment Expressions \"The Walrus Operator\" <code>:=</code>\n",
    ":class: note\n",
    "Since Python 3.8, there is a new operator for \"assignment expressions\" called \"[The Walrus Operator](https://peps.python.org/pep-0572/)\" which allows variables to be assigned inside other expressions. We've used it here to trap the value at time of error and save space. Do not feel compelled to use this yourself, especially if it's not clear what is happening.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41eb748",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<p class=\"exercise-title\"> Check your knowledge: Validator Basics\n",
    "    <p>How would you validate that <code>symbols</code> entries are at most 2 characters? There is more than one correct solution beyond what we show here.</p>\n",
    "\n",
    "```{admonition} Possible Solution:\n",
    ":class: dropdown\n",
    "```python\n",
    "@validator(\"symbols\")\n",
    "def symbols_are_possible_element_length(cls, symbs):\n",
    "    if not all(1 <= len(failure := symb) <= 2 for symb in symbs):\n",
    "        raise ValueError(f\"Symbols be 1 or 2 characters, got {failure}\")\n",
    "    return symbs\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8f496",
   "metadata": {},
   "source": [
    "## Validating against other fields\n",
    "\n",
    "*pydantic*'s validators can check fields beyond their own. This is helpful for cross referencing dependent data. In our example, we want to make sure there are exactly the right number of `coordinates` as there are `symbols` in our `Molecule`. To check against other fields in a `validator`, we extend the arguments to include one called `values`. We are going to leave our initial validator to show a feature of the `validator`s for now, but we could combine them (and will) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bf0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_match_symbols(cls, coords, values):\n",
    "        n_symbols = len(values[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e075d",
   "metadata": {},
   "source": [
    "We've added a second validator to our code called `ensure_coordinates_match_symbols`, and this funciton will validate against `coordinates`. There are two main things we can see from adding this function:\n",
    "\n",
    "1. Multiple functions can be declared to validate against the same field.\n",
    "2. We've added a one of the blocked argument names to our new validator: `values`.\n",
    "\n",
    "The reason the blocked argument names were given in the list of rules for `validators` is because *pydantic*'s `validator` reserves those to inject special code. The addition of `values` as an argument tells the `validator` to also retrieve *all previously validated fields for the model*. In our case, that would be `name`, `charge`, and `symbols` as those entries appeared before `coordinates` in the list of attributes. Any and all validators which would have been applied to those three entries have already been done and what we have access to is their validated records as a dictionary called `values` in the function itself. [See the *pydantic* docs](https://pydantic-docs.helpmanual.io/usage/validators/) for more details about the special arguments in `validator`.\n",
    "\n",
    "Let's see this in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab165cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  There must be an equal number of XYZ coordinates as there are symbols. There are 2 coordinates and 3 symbols. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m good_water \u001b[38;5;241m=\u001b[39m Molecule(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data)\n\u001b[1;32m      2\u001b[0m mangled \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_symbols_and_cords}\n\u001b[0;32m----> 3\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  There must be an equal number of XYZ coordinates as there are symbols. There are 2 coordinates and 3 symbols. (type=value_error)"
     ]
    }
   ],
   "source": [
    "good_water = Molecule(**mol_data)\n",
    "mangled = {**mol_data, **bad_symbols_and_cords}\n",
    "water = Molecule(**mangled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a090b",
   "metadata": {},
   "source": [
    "## Non-native Types in Pydantic\n",
    "\n",
    "Scientific data does not, and often should not, be confined to native Python types. One of the most common data types, especially in the sciences, is the NumPy Array (`ndarray` class). The most natural place for this would be `coordinates` where we want to simplify this list of list construct. Let's see what happens when we try to just make the type annotation a `ndarray` and see how *pydantic* handles coercion, or how it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee77aaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no validator found for <class 'numpy.ndarray'>, see `arbitrary_types_allowed` in Config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, validator\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMolecule\u001b[39;00m(BaseModel):\n\u001b[1;32m      5\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      6\u001b[0m     charge: \u001b[38;5;28mfloat\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:205\u001b[0m, in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/fields.py:491\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/fields.py:421\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/fields.py:542\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/fields.py:804\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.populate_validators\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/validators.py:723\u001b[0m, in \u001b[0;36mfind_validators\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no validator found for <class 'numpy.ndarray'>, see `arbitrary_types_allowed` in Config"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_match_symbols(cls, coords, values):\n",
    "        n_symbols = len(values[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df0862",
   "metadata": {},
   "source": [
    "This error was thrown because *pydantic* is coded to handle certain types of data, but it cannot handle types it was not programmed to understand. However, *pydantic* does provide a useful error message to fix this.\n",
    "\n",
    "You can configure your *pydantic* models to modify their behavior by adding a class within the `BaseModel` class explicitly called `Config`. This is not an imported object, its just a class bearing that name. Within that class, you set class attributes that serve as the options.\n",
    "\n",
    "```{admonition} More Config settings\n",
    ":class: note\n",
    "You can see all of the config settings [in the *pydantic* docs](https://pydantic-docs.helpmanual.io/usage/model_config/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d76dd7",
   "metadata": {},
   "source": [
    "Our particular error is saying we need to configure our model and set `arbitrary_types_allowed`, in this case to `True`. This will tell this particular `BaseModel` to permit types that it does not naturally understand how to handle, and assume the user/programer will handle it. Let's see what `Molecule` looks like with this set. Note: The location of the `class Config` statement does not matter, and `Config` is on a per-model basis, not a global *pydantic* configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ae5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_match_symbols(cls, coords, values):\n",
    "        n_symbols = len(values[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df01080",
   "metadata": {},
   "source": [
    "Our model is now configured to allow arbitrary types; no more error. Let's see what happens when we pass in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be695f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  instance of ndarray expected (type=type_error.arbitrary_type; expected_arbitrary_type=ndarray)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmol_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  instance of ndarray expected (type=type_error.arbitrary_type; expected_arbitrary_type=ndarray)"
     ]
    }
   ],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b176646",
   "metadata": {},
   "source": [
    "We're still getting a validation error, but it's different. *pydantic* is now telling us that the data given to `coordinates` must be of type `ndarray`. Remember there are two default levels of validation in *pydantic*: Ensure type, manually written validators. When we have `arbitrary_types_allowed` configured, any unknown type to *pydantic* is not type-checked or coerced beyond that it is the declared type. Effectively, a glorified `isinstance` check.\n",
    "\n",
    "So to fix this, either the user has to have already cast the data to the expected type, or the developer has to preempt the type validation somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bffe4",
   "metadata": {},
   "source": [
    "## Pre-Validators in Pydantic\n",
    "\n",
    "Good news! You can make *pydantic* validators that run before the type validation, effectively adding a third layer of validation stack. These are called \"pre-validators\" and will run before any other level of validator. The primary use case for these validators is data coercion, and that includes casting incoming data to specific types. E.g. Casting a list of lists to a NumPy array because we have `arbitrary_types_allowed` set.\n",
    "\n",
    "A pre-validator is defined exactly like any other `validator`, it just has the keyword `pre=True` in its arguments. We're going to use the validator to take the `coordinates` data in, and cast it to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560b0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    @validator(\"coordinates\", pre=True)\n",
    "    def coord_to_numpy(cls, coords):\n",
    "        try:\n",
    "            coords = np.asarray(coords)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {coords} to numpy array\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_match_symbols(cls, coords, values):\n",
    "        n_symbols = len(values[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e080a",
   "metadata": {},
   "source": [
    "Now we can see what happens when we run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ad6a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water = Molecule(**mol_data)\n",
    "water.coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc4292",
   "metadata": {},
   "source": [
    "We now have a NumPy array for our `coordinates`. Since we now have a NumPy array for `coordinates`, we can refine the original `validator`s. We'll condense our normal `coordinates` `validator`s down to a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02e4c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    @validator(\"coordinates\", pre=True)\n",
    "    def coord_to_numpy(cls, coords):\n",
    "        try:\n",
    "            coords = np.asarray(coords)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {coords} to numpy array\")\n",
    "        return coords\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def coords_length_of_symbols(cls, coords, values):\n",
    "        symbols = values[\"symbols\"]\n",
    "        if (len(coords.shape) != 2) or (len(symbols) != coords.shape[0]) or (coords.shape[1] != 3):\n",
    "            raise ValueError(f\"Coordinates must be of shape [Number Symbols, 3], was {coords.shape}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b8a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0420a0cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for Molecule\ncharge\n  value is not a valid float (type=type_error.float)\ncoordinates\n  Coordinates must be of shape [Number Symbols, 3], was (3,) (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mangle \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_charge, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_coords}\n\u001b[0;32m----> 2\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Molecule\ncharge\n  value is not a valid float (type=type_error.float)\ncoordinates\n  Coordinates must be of shape [Number Symbols, 3], was (3,) (type=value_error)"
     ]
    }
   ],
   "source": [
    "mangle = {**mol_data, **bad_charge, **bad_coords}\n",
    "water = Molecule(**mangle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a700e6a",
   "metadata": {},
   "source": [
    "We've now upgraded our `Molecule` with more advanced data validation leaning into scientific validity, added in custom types which increase our model's usability, and configured our model to further expand our capabilities. The code is now at the Lesson Materials labeled `05_valid_pydantic_molecule.py`.\n",
    "\n",
    "Next chapter we'll look at nesting models to allow more complicated data structures. \n",
    "\n",
    "Below is a supplementary section on how you can define custom, non-native types without `arbitrary_types_allowed`, giving you greater control over defining custom or even shorthand types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dfced",
   "metadata": {},
   "source": [
    "## Supplemental: Defining Custom Types with Built-In Validators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32442295",
   "metadata": {},
   "source": [
    "In the example of this chapter, we showed how to combine `arbitrary_types_allowed` in `Config` with the `validator(..., pre=True)` to convert incoming data to the types not understood by *pydantic*. There are obvious limitations to this such as having to write a different set of validators for each Model, being limited (or at least confined) in how you can permit types through, and then having to be accepting of arbitrary types.\n",
    "\n",
    "*pydantic* provides a separate way to write your custom class validator by extending the class in question. This can be done even to extend existing known types to augment them to special conditions. \n",
    "\n",
    "```{admonition} Pydantic example: Regular Expression Based String Extension\n",
    ":class: note\n",
    "The pydantic site has an example of [validating a string that is a UK postcode](https://pydantic-docs.helpmanual.io/usage/types/#classes-with-__get_validators__) on their site, which creates a custom validator for <code>str</code> type. Check it out for more examples.\n",
    "```\n",
    "\n",
    "Let's extend a NumPy array type to have be something *pydantic* can validate without needing to use `arbitrary_types_allowed`. The main thing you need is to make a subclass of the type in question, then create a `classmethod` called `__get_validators__` that takes no arguments, and yields a series of `classmethod` validation functions. But talk is cheap and examples are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0994c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ValidatableArray(np.ndarray):\n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.cast_to_ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def cast_to_ndarray(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81250fc6",
   "metadata": {},
   "source": [
    "That's it. We've defined our subclass of `ndarray` called `ValidatableArray`. We added a `classmethod` called `__get_validators__` which accepts no argument and yields a `classfunctions` to handle the validation. We only have one function here, so there is only one object to `yield`. If there were multiple objects to yield, each object would accept the validated/coerced value from the one before it. Because these are all `classmethod`s and not called on an instance of the `ValidatableArray`, we don't have to do anything with the validated value inside `__get_validators__`; its all handled by a routine in *pydantic*.\n",
    "\n",
    "Let's apply this to our `Molecule`.\n",
    "\n",
    "```{admonition} This won't appear in the next chapter\n",
    ":class: note\n",
    "The main Lesson Materials will not have this modification since this is all supplemental. Next chapter will start with the <code>05_valid_pydantic_molecule.py</code> Lesson Materials.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "078e4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: ValidatableArray\n",
    "        \n",
    "    @validator(\"coordinates\")\n",
    "    def coords_length_of_symbols(cls, coords, values):\n",
    "        symbols = values[\"symbols\"]\n",
    "        if (len(coords.shape) != 2) or (len(symbols) != coords.shape[0]) or (coords.shape[1] != 3):\n",
    "            raise ValueError(f\"Coordinates must be of shape [Number Symbols, 3], was {coords.shape}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d0b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e68474b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for Molecule\ncharge\n  value is not a valid float (type=type_error.float)\ncoordinates\n  Coordinates must be of shape [Number Symbols, 3], was (3,) (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mangle \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_charge, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_coords}\n\u001b[0;32m----> 2\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Molecule\ncharge\n  value is not a valid float (type=type_error.float)\ncoordinates\n  Coordinates must be of shape [Number Symbols, 3], was (3,) (type=value_error)"
     ]
    }
   ],
   "source": [
    "mangle = {**mol_data, **bad_charge, **bad_coords}\n",
    "water = Molecule(**mangle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ad102",
   "metadata": {},
   "source": [
    "We removed the `Config` since we no longer are handling arbitrary types: we're handling the explicit type we defined. We also removed the `pre=True` validator on `coordinates` because that work got pushed to the `ValidatableArray`. That new subclass we wrote already preempts our custom `coords_length_of_symbols` `validator` because it operates at the same time as the type annotation check, which comes before custom validators in order of operations.\n",
    "\n",
    "If we wanted to make a custom schema output for our new type, we would need to add another class method called `__modify_schema__`. However, please refer to the [*pydantic* docs](https://pydantic-docs.helpmanual.io/usage/types/#classes-with-__get_validators__) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38234556",
   "metadata": {},
   "source": [
    "## Supplemental: Defining Custom NumPy Type AND Setting Data Type (*dtype*)\n",
    "\n",
    "It is possible to set the NumPy array `dtype` as well as part of the type checking without having to define multiple custom types. This approach is not related to *pydantic* per se, but is a showcase of chaining several very advanced Python topics together.\n",
    "\n",
    "In the previous Supplemental, we showed how to write a subclass with `__get_validators__` to define a NumPy `ndarray` type in *pydantic*. We cast the input data to a numpy array with the `np.asarray`. That function can also accept a `dtype=...` argument where you can specify the type of data the array will be. How would you support arbitrarily setting the `dtype`?\n",
    "\n",
    "There are several, equally acceptable and perfectly valid, approaches to this. \n",
    "\n",
    "### Multiple Validators\n",
    "\n",
    "One option would be to make multiple types of validators and call the one you need. And there are several ways to do this. The first way is to just make multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c5b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntArray(np.ndarray):\n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.cast_to_ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def cast_to_ndarray(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=int)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "    \n",
    "class FloatArray(np.ndarray):\n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.cast_to_ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def cast_to_ndarray(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=float)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e81fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: IntArray\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: FloatArray\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2b0ae",
   "metadata": {},
   "source": [
    "A valid approach, can be dropped in when needed. However, this involves code duplication. \n",
    "\n",
    "We can cut down on the work by defining a top level class and then inheriting and subclassing it as needed through Monkey Patching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e2707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseArray(np.ndarray):\n",
    "    _dtype = None\n",
    "    \n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.cast_to_ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def cast_to_ndarray(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=cls._dtype)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "\n",
    "class InttArray(BaseArray):\n",
    "    _dtype = int\n",
    "    \n",
    "class FloatArray(BaseArray):\n",
    "    _dtype = float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4cb366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: IntArray\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: FloatArray\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796fb0d",
   "metadata": {},
   "source": [
    "### Make an on Demand Typer Function\n",
    "\n",
    "One option is to just make a function create types on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4882ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseArray(np.ndarray):\n",
    "    _dtype = None\n",
    "    \n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.cast_to_ndarray\n",
    "\n",
    "    @classmethod\n",
    "    def cast_to_ndarray(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=cls._dtype)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "\n",
    "def array_typer(dtype):\n",
    "    class GeneratedType(BaseArray):\n",
    "        _dtype = dtype\n",
    "    return GeneratedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6566076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: array_typer(int)\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: array_typer(float)\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc69c33",
   "metadata": {},
   "source": [
    "But this has the problem of now having to regenerate a new class each time, and its type schema will always be \"GeneratedType\" class. This isn't a problem most of the time, but it can be a little confusing to suddenly see functions in type annotation instead of the normal types and square brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6f51e",
   "metadata": {},
   "source": [
    "### Metclass for on-the-fly assignment.\n",
    "\n",
    "One option MolSSI deploys in our [`QCElemental` package](https://github.com/MolSSI/QCElemental/blob/295642189fe3c4d0812142c0304d8ae9c8674d4c/qcelemental/models/types.py#L39) is to use a [Python Metaclass](https://docs.python.org/3/reference/datamodel.html#metaclasses) as a way to define a class generator whose properties are set dynamically, then usable by the class. \n",
    "\n",
    "```{admonition} Here There be Forbidden Magics\n",
    ":class: warning\n",
    "“Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don’t (the people who actually need them know with certainty that they need them, and don’t need an explanation about why).”\n",
    "\n",
    "— Tim Peters, Author of [Zen of Python, PEP 20](https://peps.python.org/pep-0020/)\n",
    "```\n",
    "\n",
    "Metaclasses are usually not something you want to touch, because you don't need to. The above methods provide a fine way to generate type hints dynamically. However, if you want to be fancy, you can use a Metaclass. The best primer I, Levi Naden, have found on Metaclasses at the time of writing this section (Fall 2022) was through [this Stack Overflow answer](https://stackoverflow.com/a/6581949/10364409).\n",
    "\n",
    "For our example, we're going to define a base typed array, define a Metaclass which abuses the `__getindex__` to treat our `[ ]` arguments for \"type hint\" as a type assignment, then feed the metaclass in with that type specification as a settable parameter and generate classes on the fly.\n",
    "\n",
    "Let's see that in code where we've annotated the lines everything in the last paragraph said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base typed array\n",
    "class TypedArray(np.ndarray):    \n",
    "    _dtype = None\n",
    "    \n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.validate\n",
    "\n",
    "    @classmethod\n",
    "    def validate(cls, v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=cls._dtype)\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Could not cast {} to NumPy Array!\".format(v))\n",
    "        return v\n",
    "\n",
    "# A Metaclass which abuses the `__getindex__` to treat our `[ ]` arguments for \"type hint\" as a type assignment\n",
    "class ArrayMeta(type):\n",
    "    def __getitem__(self, dtype):\n",
    "        # Feed the metaclass in with that type specification as a settable parameter\n",
    "        return type(\"Array\", (TypedArray,), {\"_dtype\": dtype})\n",
    "\n",
    "# Generate classes on the fly.\n",
    "class Array(np.ndarray, metaclass=ArrayMeta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3b911",
   "metadata": {},
   "source": [
    "Then in practice, we just \"index\" the `Array` class as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6172c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: Array[int]\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: Array[float]\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33deb791",
   "metadata": {},
   "source": [
    "There are also a couple downsides to this. You have to understand Metaclasses, which we showed a quote warning against. You cannot just use a bare `Array` because then it's an arbitrary type. Lastly, if you want default `dtype` handling you have to use the `TypedArray` as the type annotation or `Array[None]`, which can be a bit confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac7d83",
   "metadata": {},
   "source": [
    "### Do what makes sense, and only if you need to\n",
    "\n",
    "All of these methods are equally valid, with upsides and downsides alike. Your use case may not even need `dtype` specification and you can just accept the normal NumPy handling of casting to array plus your own custom `validator` functions to make sure the data look correct. Hopefully though this supplemental section has given you ideas to inspire your own code design and give you ideas on interesting and helpful things you can do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
