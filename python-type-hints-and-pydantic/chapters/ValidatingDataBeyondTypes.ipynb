{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25895379",
   "metadata": {},
   "source": [
    "(ch05)=\n",
    "# Validating Data Beyond Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe112157",
   "metadata": {},
   "source": [
    "```{admonition} Starting File: <code>04_pydantic_molecule.py</code>\n",
    ":class: important\n",
    "This chapter will start from the <code>04_pydantic_molecule.py</code> and end on the <code>05_valid_pydantic_molecule.py</code>.\n",
    "```\n",
    "\n",
    "Data validation goes far beyond just type. *Pydantic* has provided the basic tools for doing data validation on data types, but it also provides the tools for writing custom validators to check so much more.\n",
    "\n",
    "We'll be covering the *pydantic* `validator` decorator and applying that to our data to check structure and scientific rigor. We'll also cover how to validate types not native to Python, such as NumPy arrays.\n",
    "\n",
    "```{admonition} Check Out Pydantic\n",
    ":class: note\n",
    "We will not be covering all the capabilities of *pydantic* here, and we highly encourage you to visit [the pydantic docs](https://pydantic-docs.helpmanual.io/) to learn about all the powerful and easy-to-execute things *pydantic* can do.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Compatibility with Python 3.8 and below\n",
    ":class: note\n",
    "If you have Python 3.8 or below, you will need to import container type objects such as `List`, `Tuple`, `Dict`, etc. from the `typing` library instead of their native types of `list`, `tuple`, `dict`, etc. This chapter will assume Python 3.9 or greater, however, both approaches will work in >=Python 3.9 and have 1:1 replacements of the same name.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd72c9",
   "metadata": {},
   "source": [
    "## Pydantic's Validator Decorator\n",
    "\n",
    "Let's start by looking at the state of our code prior to extending the validators. As usual, let's also define our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f35ea97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "\n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d440290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_data = {  # Good data\n",
    "    \"coordinates\": [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "    \"symbols\": [\"H\", \"H\", \"O\"], \n",
    "    \"charge\": 0.0, \n",
    "    \"name\": \"water\"\n",
    "}\n",
    "\n",
    "bad_name = {\"name\": 789}  # Name is not str\n",
    "bad_charge = {\"charge\": [1, 0.0]}  # Charge is not int or float\n",
    "noniter_symbols = {\"symbols\": 1234567890}  # Symbols is an int\n",
    "nonlist_symbols = {\"symbols\": '[\"H\", \"H\", \"O\"]'}  # Symbols is a string (notably is a string-ified list)\n",
    "tuple_symbols = {\"symbols\": (\"H\", \"H\", \"O\")}  # Symbols as a tuple?\n",
    "bad_coords = {\"coordinates\": [\"1\", \"2\", \"3\"]}  # Coords is a single list of string\n",
    "inner_coords_not3d = {\"coordinates\": [[1, 2, 3], [4, 5]]}\n",
    "bad_symbols_and_cords = {\"symbols\": [\"H\", \"H\", \"O\"],\n",
    "                         \"coordinates\": [[1, 1, 1], [2.0, 2.0, 2.0]]\n",
    "                        }  # Coordinates top-level list is not the same length as symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9a19b",
   "metadata": {},
   "source": [
    "You may notice we have extended our \"Good Data\" here to have `coordinates` actually define the `Nx3` structure where `N = len(symbols)`. This is important for what we plan to validate.\n",
    "\n",
    "*pydantic* allows you to write custom validators, in addition to the type validators which run automatically for a type annotation. This `field_validator` is pulled from the `pydantic` module just like `BaseModel`, and is used to decorate a *class* function you write. Let's look at the most basic `field_validator` we can write and assign it to `coordinates`.\n",
    "\n",
    "```{admonition} Field vs Annotated Validators\n",
    ":class: note\n",
    "`pydantic` allows validators to be defined functionally for reuse, ordering, and much more powerful utilization through the `Annotated` class. We will be showing `field_validator` for this example to keep the validator much more local for learning purposes. Please see (the *pydantic* docs on validators for more info.)[https://docs.pydantic.dev/latest/usage/validators/]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "229837c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        return coords\n",
    "\n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa0030",
   "metadata": {},
   "source": [
    "Here we have defined an additional validator which does nothing, but has the basic structure we can look at. For convenience and reference, I've broken the aspects of the `field_validator` into a list.\n",
    "\n",
    "* The `field_validator` decorator takes as arguments the *exact* name of the attributes you are validating against as a string. In this case `coordinates`. You could provide multiple string args of each attribute you want to run through the validator if you want to reuse it.\n",
    "* The function name can be whatever you want it to be. We've called it `ensure_coordinates_is_3D` to be meaningful if anyone ever wants to come back and see what this should be doing.\n",
    "* The function itself is a *class function*. This is why we have included the `@classmethod` decorator from native Python, this validator is intended to be called on the non-instanced class. The formal nomenclature for the first variable here is therefore `cls` and not `self`. You can define the validators without the `@classmethod` decorator, but your IDE may complain about this, so we also add the `@classmethod` decorator so we can use `cls` without IDE issues, at least on that point.\n",
    "* The first (non `cls`) argument of the function can be whatever string name you want. The **optional** second argument will be give a *pydantic* metadata class of type `FieldValidationInfo` and can also be named whatever we want. We'll use this metadata class later in the chapter.\n",
    "* The return MUST be the validated data to be fed into the attribute. We've done nothing to our variable `coords`, so we simply return it. If you fail to have a `return` statement with something, it will return `None` and that will be considered valid.\n",
    "* If the data are not validated correctly, the function must raise either a `ValueError` or `AssertionError` for *pydantic* to correctly trap the error, anything else will raise the Python error stack as normal.\n",
    "* `field_validator` runs *after* type validation, unless specified (see later in this chapter).\n",
    "\n",
    "That may seem like lots of rules, but most of them are boilerplate and intuitive. Let's apply these items to our validator. We want to make sure the inner lists of `coordinates` are 3D, or length 3. We don't have to worry about type checking (that was done before any custom `field_validator` was run), so we can just do an iteration of the top list and make sure. Let's apply that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94a2e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b8cb3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  Value error, Inner coordinates must be 3D, got [4.0, 5.0] of length 2 [type=value_error, input_value=[[1, 2, 3], [4, 5]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m good_water \u001b[38;5;241m=\u001b[39m Molecule(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data)\n\u001b[1;32m      2\u001b[0m mangled \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_coords_not3d}\n\u001b[0;32m----> 3\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  Value error, Inner coordinates must be 3D, got [4.0, 5.0] of length 2 [type=value_error, input_value=[[1, 2, 3], [4, 5]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error"
     ]
    }
   ],
   "source": [
    "good_water = Molecule(**mol_data)\n",
    "mangled = {**mol_data, **inner_coords_not3d}\n",
    "water = Molecule(**mangled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5196d1e",
   "metadata": {},
   "source": [
    "Here we have checked the good data still works, and checked that the mangled data raised an error. It's important to note the error raised by the function was a `ValueError` (or `AssertionError`) so the error report was a `ValidationError`. We can also see the error message is what we put as the error string and `type` of error is of the type we raised. This is why it's very important to have meaningful error strings when your custom validator fails.\n",
    "\n",
    "With all that said, our validator function really does look like any other function we may call to do a quick check of data, and then some special addons to make it work with *pydantic*. There is no practical limit to the number of `field_validator`s you have in a given class, so validate to your heart's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39d716",
   "metadata": {},
   "source": [
    "```{admonition} Python Assignment Expressions \"The Walrus Operator\" <code>:=</code>\n",
    ":class: note\n",
    "Since Python 3.8, there is a new operator for \"assignment expressions\" called \"[The Walrus Operator](https://peps.python.org/pep-0572/)\" which allows variables to be assigned inside other expressions. We've used it here to trap the value at time of error and save space. Do not feel compelled to use this yourself, especially if it's not clear what is happening.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41eb748",
   "metadata": {},
   "source": [
    "<div class=\"exercise\">\n",
    "<p class=\"exercise-title\"> Check your knowledge: Validator Basics\n",
    "    <p>How would you validate that <code>symbols</code> entries are at most 2 characters? There is more than one correct solution beyond what we show here.</p>\n",
    "\n",
    "```{admonition} Possible Solution:\n",
    ":class: dropdown\n",
    "```python\n",
    "@field_validator(\"symbols\")\n",
    "@classmethod\n",
    "def symbols_are_possible_element_length(cls, symbs):\n",
    "    if not all(1 <= len(failure := symb) <= 2 for symb in symbs):\n",
    "        raise ValueError(f\"Symbols be 1 or 2 characters, got {failure}\")\n",
    "    return symbs\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8f496",
   "metadata": {},
   "source": [
    "## Validating against other fields\n",
    "\n",
    "*pydantic*'s validators can check fields beyond their own. This is helpful for cross referencing dependent data. In our example, we want to make sure there are exactly the right number of `coordinates` as there are `symbols` in our `Molecule`. To check against other fields in a `field_validator`, we extend the arguments to include the optional secondary one for metadata we're going to call `info`. We are going to leave our initial validator to show a feature of the `field_validator`s for now, but we could combine them (and will) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "14bf0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: list[list[float]]\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_match_symbols(cls, coords, info):\n",
    "        n_symbols = len(info.data[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e075d",
   "metadata": {},
   "source": [
    "We've added a second validator to our code called `ensure_coordinates_match_symbols`, and this funciton will validate against `coordinates`. There are two main things we can see from adding this function:\n",
    "\n",
    "1. Multiple functions can be declared to validate against the same field.\n",
    "2. We've added the additional optional metadata argument to our new validator: `info`.\n",
    "\n",
    "The second argument, if it appears in a `field_validator`, provides metadata for the validation currently happening and that has already happened. The addition of `info` as an argument tells the `field_validator` to also retrieve *all previously validated fields for the model*. In our case, that would be `name`, `charge`, and `symbols` as those entries appeared before `coordinates` in the list of attributes. Any and all validators which would have been applied to those three entries have already been done and what we have access to is their validated records as metadata object with those validated values stored in the dictionary at `.data`. [See the *pydantic* docs](https://docs.pydantic.dev/latest/usage/validators/) for more details about the special argument and metadata for `field_validator`.\n",
    "\n",
    "Let's see this in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ab165cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  Value error, There must be an equal number of XYZ coordinates as there are symbols. There are 2 coordinates and 3 symbols. [type=value_error, input_value=[[1, 1, 1], [2.0, 2.0, 2.0]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m good_water \u001b[38;5;241m=\u001b[39m Molecule(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data)\n\u001b[1;32m      2\u001b[0m mangled \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_symbols_and_cords}\n\u001b[0;32m----> 3\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  Value error, There must be an equal number of XYZ coordinates as there are symbols. There are 2 coordinates and 3 symbols. [type=value_error, input_value=[[1, 1, 1], [2.0, 2.0, 2.0]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error"
     ]
    }
   ],
   "source": [
    "good_water = Molecule(**mol_data)\n",
    "mangled = {**mol_data, **bad_symbols_and_cords}\n",
    "water = Molecule(**mangled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a090b",
   "metadata": {},
   "source": [
    "## Non-native Types in Pydantic\n",
    "\n",
    "Scientific data does not, and often should not, be confined to native Python types. One of the most common data types, especially in the sciences, is the NumPy Array (`ndarray` class). The most natural place for this would be `coordinates` where we want to simplify this list of list construct. Let's see what happens when we try to just make the type annotation a `ndarray` and see how *pydantic* handles coercion, or how it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ee77aaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticSchemaGenerationError",
     "evalue": "Unable to generate pydantic-core schema for <class 'numpy.ndarray'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.0.3/u/schema-for-unknown-type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticSchemaGenerationError\u001b[0m             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, field_validator\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMolecule\u001b[39;00m(BaseModel):\n\u001b[1;32m      5\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      6\u001b[0m     charge: \u001b[38;5;28mfloat\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:174\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m types_namespace \u001b[38;5;241m=\u001b[39m get_cls_types_namespace(\u001b[38;5;28mcls\u001b[39m, parent_namespace)\n\u001b[1;32m    173\u001b[0m set_model_fields(\u001b[38;5;28mcls\u001b[39m, bases, config_wrapper, types_namespace)\n\u001b[0;32m--> 174\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# using super(cls, cls) on the next line ensures we only call the parent class's __pydantic_init_subclass__\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# I believe the `type: ignore` is only necessary because mypy doesn't realize that this code branch is\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# only hit for _proper_ subclasses of BaseModel\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39m__pydantic_init_subclass__(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:431\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace)\u001b[0m\n\u001b[1;32m    424\u001b[0m handler \u001b[38;5;241m=\u001b[39m CallbackGetCoreSchemaHandler(\n\u001b[1;32m    425\u001b[0m     partial(gen_schema\u001b[38;5;241m.\u001b[39mgenerate_schema, from_dunder_get_core_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    426\u001b[0m     gen_schema,\n\u001b[1;32m    427\u001b[0m     ref_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munpack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    428\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_pydantic_core_schema__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:533\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[0;34m(cls, _BaseModel__source, _BaseModel__handler)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__source\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_schema_generation_shared.py:82\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, _CallbackGetCoreSchemaHandler__source_type)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, __source_type: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 82\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__source_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:280\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema, from_prepare_args)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtype\u001b[39m(Annotated[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m123\u001b[39m])):\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_annotated_schema(obj)\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_for_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_dunder_get_core_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_dunder_get_core_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_prepare_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_prepare_args\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:301\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_for_type\u001b[0;34m(self, obj, from_dunder_get_core_schema, from_prepare_args)\u001b[0m\n\u001b[1;32m    298\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:519\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:370\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_wrapper_stack\u001b[38;5;241m.\u001b[39mappend(config_wrapper)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 370\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_md_field_schema(k, v, decorators) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    371\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m decorators\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[1;32m    372\u001b[0m         extra_validator\u001b[38;5;241m=\u001b[39mextra_validator,\n\u001b[1;32m    373\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_wrapper_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:370\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_wrapper_stack\u001b[38;5;241m.\u001b[39mappend(config_wrapper)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 370\u001b[0m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    371\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m decorators\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[1;32m    372\u001b[0m         extra_validator\u001b[38;5;241m=\u001b[39mextra_validator,\n\u001b[1;32m    373\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_wrapper_stack\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:674\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    669\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    670\u001b[0m     field_info: FieldInfo,\n\u001b[1;32m    671\u001b[0m     decorators: DecoratorInfos,\n\u001b[1;32m    672\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[1;32m    676\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    677\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    681\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    682\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:714\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m    712\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(source_type, annotations, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[1;32m    723\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:1405\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[0;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m annotations[idx]\n\u001b[1;32m   1401\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[1;32m   1402\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[1;32m   1403\u001b[0m     )\n\u001b[0;32m-> 1405\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[1;32m   1407\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_schema_generation_shared.py:82\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, _CallbackGetCoreSchemaHandler__source_type)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, __source_type: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 82\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__source_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:1366\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1364\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, obj)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1366\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1368\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:586\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_alias_type_schema(obj)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arbitrary_type_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# Need to handle generic dataclasses before looking for the schema properties because attribute accesses\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# on _GenericAlias delegate to the origin type, so lose the information about the concrete parametrization\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# As a result, currently, there is no way to cache the schema for generic dataclasses. This may be possible\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# to resolve by modifying the value returned by `Generic.__class_getitem__`, but that is a dangerous game.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39mis_dataclass(origin):\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:638\u001b[0m, in \u001b[0;36mGenerateSchema._arbitrary_type_schema\u001b[0;34m(self, obj, type_)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mis_instance_schema(type_)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticSchemaGenerationError(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to generate pydantic-core schema for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSet `arbitrary_types_allowed=True` in the model_config to ignore this error\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or implement `__get_pydantic_core_schema__` on your type to fully support it.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you got this error by calling handler(<some type>) within\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `__get_pydantic_core_schema__` then you likely need to call\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `handler.generate_schema(<some type>)` since we do not call\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    646\u001b[0m     )\n",
      "\u001b[0;31mPydanticSchemaGenerationError\u001b[0m: Unable to generate pydantic-core schema for <class 'numpy.ndarray'>. Set `arbitrary_types_allowed=True` in the model_config to ignore this error or implement `__get_pydantic_core_schema__` on your type to fully support it.\n\nIf you got this error by calling handler(<some type>) within `__get_pydantic_core_schema__` then you likely need to call `handler.generate_schema(<some type>)` since we do not call `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.\n\nFor further information visit https://errors.pydantic.dev/2.0.3/u/schema-for-unknown-type"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_match_symbols(cls, coords, info):\n",
    "        n_symbols = len(info.data[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df0862",
   "metadata": {},
   "source": [
    "This error was thrown because *pydantic* is coded to handle certain types of data, but it cannot handle types it was not programmed to understand. However, *pydantic* does provide a useful error message to fix this.\n",
    "\n",
    "You can configure your *pydantic* models to modify their behavior by adding a class attribute within the `BaseModel` class explicitly called `model_config` that is an instance of the `ConfigDict` class provided by *pydantic*. Within that class, you set class keywords which serve as settings for the model they are attached to.\n",
    "\n",
    "```{admonition} More model_config settings\n",
    ":class: note\n",
    "You can see all of the config settings [in the *pydantic* docs](https://docs.pydantic.dev/latest/usage/model_config/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d76dd7",
   "metadata": {},
   "source": [
    "Our particular error says many things, but we are going to focus on the simplest where it says we need to configure our model and set `arbitrary_types_allowed`, in this case to `True`. This will tell this particular `BaseModel` to permit types that it does not naturally understand how to handle, and assume the user/programer will handle it. Let's see what `Molecule` looks like with this set. Note: The location of the `model_config` attribute does not matter, and `model_config` is on a per-model basis, not a global *pydantic* configuration.\n",
    "\n",
    "```{admonition} Better and more powerful ways to do this with pydantic\n",
    ":class: note\n",
    "Pydantic has much more powerful and precise ways to establish custom types than what we show here! Treat this lesson as a rudimentary basics in understanding custom types and only *some* of the ways to validate them. Please [see the pydantic docs on custom validation](https://docs.pydantic.dev/latest/usage/types/custom/) which includes examples on how to handle third-party types such as NumPy or Pandas.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43ae5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, field_validator, ConfigDict\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "    \n",
    "    model_config = ConfigDict(arbitrary_types_allowed = True)\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_match_symbols(cls, coords, info):\n",
    "        n_symbols = len(info.data[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df01080",
   "metadata": {},
   "source": [
    "Our model is now configured to allow arbitrary types; no more error. Let's see what happens when we pass in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be695f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Molecule\ncoordinates\n  Input should be an instance of ndarray [type=is_instance_of, input_value=[[0, 0, 0], [1, 1, 1], [2, 2, 2]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/is_instance_of",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmol_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Molecule\ncoordinates\n  Input should be an instance of ndarray [type=is_instance_of, input_value=[[0, 0, 0], [1, 1, 1], [2, 2, 2]], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/is_instance_of"
     ]
    }
   ],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b176646",
   "metadata": {},
   "source": [
    "We're still getting a validation error, but it's different. *pydantic* is now telling us that the data given to `coordinates` must be of type `ndarray`. Remember there are two default levels of validation in *pydantic*: Ensure type, manually written validators. When we have `arbitrary_types_allowed` configured, any unknown type to *pydantic* is not type-checked or coerced beyond that it is the declared type. Effectively, a glorified `isinstance` check.\n",
    "\n",
    "So to fix this, either the user has to have already cast the data to the expected type, or the developer has to preempt the type validation somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bffe4",
   "metadata": {},
   "source": [
    "## Before-Validators in Pydantic\n",
    "\n",
    "Good news! You can make *pydantic* validators that run before the type validation, effectively adding a third layer of validation stack. These are called \"before validators\" and will run before any other level of validator. The primary use case for these validators is data coercion, and that includes casting incoming data to specific types. E.g. Casting a list of lists to a NumPy array because we have `arbitrary_types_allowed` set.\n",
    "\n",
    "A pre-validator is defined exactly like any other `field_validator`, it just has the keyword `mode='before'` in its arguments. We're going to use the validator to take the `coordinates` data in, and cast it to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "560b0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, field_validator, ConfigDict\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    model_config = ConfigDict(arbitrary_types_allowed = True)\n",
    "    \n",
    "    @field_validator(\"coordinates\", mode='before')\n",
    "    @classmethod\n",
    "    def coord_to_numpy(cls, coords):\n",
    "        try:\n",
    "            coords = np.asarray(coords)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {coords} to numpy array\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_match_symbols(cls, coords, info):\n",
    "        n_symbols = len(info.data[\"symbols\"])\n",
    "        if (n_coords := len(coords)) != n_symbols:  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"There must be an equal number of XYZ coordinates as there are symbols.\" \n",
    "                             f\" There are {n_coords} coordinates and {n_symbols} symbols.\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def ensure_coordinates_is_3D(cls, coords):\n",
    "        if any(len(failure := inner) != 3 for inner in coords):  # Walrus operator (:=) for Python 3.8+\n",
    "            raise ValueError(f\"Inner coordinates must be 3D, got {failure} of length {len(failure)}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e080a",
   "metadata": {},
   "source": [
    "Now we can see what happens when we run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e5ad6a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water = Molecule(**mol_data)\n",
    "water.coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc4292",
   "metadata": {},
   "source": [
    "We now have a NumPy array for our `coordinates`. Since we now have a NumPy array for `coordinates`, we can refine the original `validator`s. We'll condense our normal `coordinates` `validator`s down to a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "02e4c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, field_validator, ConfigDict\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: np.ndarray\n",
    "        \n",
    "    model_config = ConfigDict(arbitrary_types_allowed = True)\n",
    "    \n",
    "    @field_validator(\"coordinates\", mode='before')\n",
    "    @classmethod\n",
    "    def coord_to_numpy(cls, coords):\n",
    "        try:\n",
    "            coords = np.asarray(coords)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {coords} to numpy array\")\n",
    "        return coords\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def coords_length_of_symbols(cls, coords, info):\n",
    "        symbols = info.data[\"symbols\"]\n",
    "        if (len(coords.shape) != 2) or (len(symbols) != coords.shape[0]) or (coords.shape[1] != 3):\n",
    "            raise ValueError(f\"Coordinates must be of shape [Number Symbols, 3], was {coords.shape}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93b8a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0420a0cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for Molecule\ncharge\n  Input should be a valid number [type=float_type, input_value=[1, 0.0], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/float_type\ncoordinates\n  Value error, Coordinates must be of shape [Number Symbols, 3], was (3,) [type=value_error, input_value=['1', '2', '3'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mangle \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_charge, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_coords}\n\u001b[0;32m----> 2\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Molecule\ncharge\n  Input should be a valid number [type=float_type, input_value=[1, 0.0], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/float_type\ncoordinates\n  Value error, Coordinates must be of shape [Number Symbols, 3], was (3,) [type=value_error, input_value=['1', '2', '3'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error"
     ]
    }
   ],
   "source": [
    "mangle = {**mol_data, **bad_charge, **bad_coords}\n",
    "water = Molecule(**mangle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a700e6a",
   "metadata": {},
   "source": [
    "We've now upgraded our `Molecule` with more advanced data validation leaning into scientific validity, added in custom types which increase our model's usability, and configured our model to further expand our capabilities. The code is now at the Lesson Materials labeled `05_valid_pydantic_molecule.py`.\n",
    "\n",
    "Next chapter we'll look at nesting models to allow more complicated data structures. \n",
    "\n",
    "Below is a supplementary section on how you can define custom, non-native types without `arbitrary_types_allowed`, giving you greater control over defining custom or even shorthand types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dfced",
   "metadata": {},
   "source": [
    "## Supplemental: Defining Custom Types with Built-In Validators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32442295",
   "metadata": {},
   "source": [
    "In the example of this chapter, we showed how to combine `arbitrary_types_allowed` in `Config` with the `field_validator(..., mode='before')` to convert incoming data to the types not understood by *pydantic*. There are obvious limitations to this such as having to write a different set of validators for each Model, being limited (or at least confined) in how you can permit types through, and then having to be accepting of arbitrary types.\n",
    "\n",
    "*pydantic* provides a separate way to write your custom class validator by extending the class in question. This can be done even to extend existing known types to augment them to special conditions. \n",
    "\n",
    "Let's extend a NumPy array type to have be something *pydantic* can validate without needing to use `arbitrary_types_allowed`. There are two ways to do this, either as an `Annotated` type where we overload *pydantic*'s type logic, or as a custom class schema generator. We'll look at the `Annotated` method which the *pydantic* docs indicate is more [stable than the custom class schema generator from an API standpoint](https://docs.pydantic.dev/latest/usage/types/custom/#customizing-validation-with-__get_pydantic_core_schema__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99e8e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "from pydantic.functional_validators import PlainValidator\n",
    "\n",
    "\n",
    "def cast_to_np(v):\n",
    "    try:\n",
    "        v = np.asarray(v)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "\n",
    "ValidatableArray = Annotated[np.ndarray, PlainValidator(cast_to_np)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fb99a",
   "metadata": {},
   "source": [
    "That's it. \n",
    "\n",
    "We've first taken the `Annotated` object from the back-ported `typing_extensions` module which will work with Python 3.7+ (`from typing import Annotated` works with Python 3.9+ for identical behavior). This object allows you to augment types with additional metadata information for IDEs and other tools such as *pydantic* without disrupting normal code behavior.\n",
    "\n",
    "Next we've taken augmented `np.ndarray` type with the *pydantic* `PlainValidator` method and passed it a function which will overwrite any of *pydantic*'s normal logic when validating the `np.ndarray`. Otherwise *pydantic* would have attempted to validate against `np.ndarray` and we'd be back where we started with the error asking about `allow_arbitrary_types`. Instead, we've usurped the normal *pydantic* logic and effectively said \"The validator for this type is the function `cast_to_np`, send the data there, and if it doesn't error, we're good.\"\n",
    "\n",
    "There is FAR more you can do with the `Annotated` object and *pydantic*, including defining multiple Before, After, and Wrap validators for any and all class attributes. For instance, there is a `BeforeValidator` which takes a functional argument as well which can be annotated into any data field that will do the same thing as `@field_validator(..., mode='before')`. However, advanced usage is best left to the [*pydantic* docs](https://docs.pydantic.dev/latest/usage/validators/)\n",
    "\n",
    "Let's apply this to our `Molecule`.\n",
    "\n",
    "```{admonition} This won't appear in the next chapter\n",
    ":class: note\n",
    "The main Lesson Materials will not have this modification since this is all supplemental. Next chapter will start with the <code>05_valid_pydantic_molecule.py</code> Lesson Materials.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "078e4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydantic import BaseModel, field_validator, ConfigDict\n",
    "\n",
    "class Molecule(BaseModel):\n",
    "    name: str\n",
    "    charge: float\n",
    "    symbols: list[str]\n",
    "    coordinates: ValidatableArray\n",
    "        \n",
    "    @field_validator(\"coordinates\")\n",
    "    @classmethod\n",
    "    def coords_length_of_symbols(cls, coords, info):\n",
    "        symbols = info.data[\"symbols\"]\n",
    "        if (len(coords.shape) != 2) or (len(symbols) != coords.shape[0]) or (coords.shape[1] != 3):\n",
    "            raise ValueError(f\"Coordinates must be of shape [Number Symbols, 3], was {coords.shape}\")\n",
    "        return coords\n",
    "    \n",
    "    @property\n",
    "    def num_atoms(self):\n",
    "        return len(self.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d0b826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "water = Molecule(**mol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e68474b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for Molecule\ncharge\n  Input should be a valid number [type=float_type, input_value=[1, 0.0], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/float_type\ncoordinates\n  Value error, Coordinates must be of shape [Number Symbols, 3], was (3,) [type=value_error, input_value=['1', '2', '3'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mangle \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmol_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_charge, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbad_coords}\n\u001b[0;32m----> 2\u001b[0m water \u001b[38;5;241m=\u001b[39m \u001b[43mMolecule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyd-tut/lib/python3.10/site-packages/pydantic/main.py:150\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    149\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__pydantic_self__\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Molecule\ncharge\n  Input should be a valid number [type=float_type, input_value=[1, 0.0], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/float_type\ncoordinates\n  Value error, Coordinates must be of shape [Number Symbols, 3], was (3,) [type=value_error, input_value=['1', '2', '3'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.0.3/v/value_error"
     ]
    }
   ],
   "source": [
    "mangle = {**mol_data, **bad_charge, **bad_coords}\n",
    "water = Molecule(**mangle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ad102",
   "metadata": {},
   "source": [
    "We removed the `model_config` since we no longer are handling arbitrary types: we're handling the explicit type we defined. We also removed the `mode='before'` validator on `coordinates` because that work got pushed to the `ValidatableArray`. That new `Annotated` type we wrote already preempts our custom `coords_length_of_symbols` `field_validator` because it operates at the same time as the type annotation check, which comes before custom validators in order of operations.\n",
    "\n",
    "If we wanted to make a custom schema output for our new type, we would need to add another class method called `__get_pydantic_core_schema__`. However, please refer to the [*pydantic* docs](https://docs.pydantic.dev/latest/usage/types/custom/#customizing-validation-with-__get_pydantic_core_schema__) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38234556",
   "metadata": {},
   "source": [
    "## Supplemental: Defining Custom NumPy Type AND Setting Data Type (*dtype*)\n",
    "\n",
    "It is possible to set the NumPy array `dtype` as well as part of the type checking without having to define multiple custom types. This approach is not related to *pydantic* per se, but is a showcase of chaining several very advanced Python topics together.\n",
    "\n",
    "In the previous Supplemental, we showed how to write an augmented type with `Annotated` to define a NumPy `ndarray` type in *pydantic*. We cast the input data to a numpy array with the `np.asarray`. That function can also accept a `dtype=...` argument where you can specify the type of data the array will be. How would you support arbitrarily setting the `dtype`?\n",
    "\n",
    "There are several, equally acceptable and perfectly valid, approaches to this. \n",
    "\n",
    "### Multiple Validators\n",
    "\n",
    "One option would be to make multiple types of validators and call the one you need. And there are several ways to do this. The first way is to just make multiple annotated types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c5b6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_np_int(v):\n",
    "    try:\n",
    "        v = np.asarray(v, dtype=int)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "\n",
    "def cast_to_np_float(v):\n",
    "    try:\n",
    "        v = np.asarray(v, dtype=float)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "\n",
    "IntArray = Annotated[np.ndarray, PlainValidator(cast_to_np_int)]\n",
    "FloatArray = Annotated[np.ndarray, PlainValidator(cast_to_np_float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e81fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: IntArray\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: FloatArray\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2b0ae",
   "metadata": {},
   "source": [
    "A valid approach, can be dropped in when needed. However, this involves code duplication. \n",
    "\n",
    "We can cut down on the work by defining a function which accepts a keyword and use `functools.partial` to lock the keyword in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95e2707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def cast_to_np(v, dtype=None):\n",
    "    try:\n",
    "        v = np.asarray(v, dtype=dtype)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "IntArray = Annotated[np.ndarray, PlainValidator(partial(cast_to_np, dtype=int))]\n",
    "FloatArray = Annotated[np.ndarray, PlainValidator(partial(cast_to_np, dtype=float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a4cb366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: IntArray\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: FloatArray\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796fb0d",
   "metadata": {},
   "source": [
    "### Make an on Demand Typer Function\n",
    "\n",
    "One option is to just make a function create types on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4882ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_typer(dtype):\n",
    "    def cast_to_np(v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=dtype)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "    return Annotated[np.ndarray, PlainValidator(cast_to_np)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6566076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "class IntMolecule(Molecule):\n",
    "    coordinates: array_typer(int)\n",
    "        \n",
    "class FloatMolecule(Molecule):\n",
    "    coordinates: array_typer(float)\n",
    "        \n",
    "print(IntMolecule(**mol_data).coordinates)\n",
    "print(FloatMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc69c33",
   "metadata": {},
   "source": [
    "But this has the problem of now having to regenerate a new `Annotated` type each time, and its type schema will always have the same signature. This isn't a problem most of the time, but it can be a little confusing to suddenly see functions in type annotation instead of the normal types and square brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6f51e",
   "metadata": {},
   "source": [
    "### Custom Core Schema\n",
    "\n",
    "We're going to take a look at the other way *pydantic* has for defining a custom type, one way they specifically suggest for NumPy in their own documentations, namely [a custom core schema](https://docs.pydantic.dev/latest/usage/types/custom/#customizing-validation-with-__get_pydantic_core_schema__). We avoided this in the previous blocks of the lessons because the *pydantic* docs say this functionality touches the underlying `pydantic-core` functionality. And while it *does* have an API (and follows semantic versioning), its also the section most likely to change according to them.\n",
    "\n",
    "We do want to look at this approach because we are abusing the `PlainValidator` a bit to overload *pydantic*'s internal type checking.\n",
    "\n",
    "We're going to build this piece by piece, with the understanding that it won't work fully until we've constructed it. Effectively: writing the instructions for *pydantic* to handle this with mostly native `pydantic-core` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90d6fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidatableArray:\n",
    "    @classmethod\n",
    "    def __get_pydantic_core_schema__(cls, source, handler):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e50eb",
   "metadata": {},
   "source": [
    "This is the primary work method for the core schema. The `__get_pydantic_core_schema__` is the method that *pydantic* will look for when validating this type of data. \n",
    "\n",
    "`source` is the class we are generating a schema for; this will generally be the same as the `cls` argument if this is a classmethod. \n",
    "\n",
    "`handler` is the call into Pydantic's internal JSON schema generation logic. Since we're writing our own schema generator for something that *Pydantic* does not natively understand, we likely won't need `source` or `handler` at all for most uses. However, we will be taking advantage of `source` and some other Python `typing` tools as well.\n",
    "\n",
    "We'll fill in everything we want to do in the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7ff508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_core import core_schema\n",
    "\n",
    "def cast_to_np(v):\n",
    "    try:\n",
    "        v = np.asarray(v)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "\n",
    "class ValidatableArray:\n",
    "    @classmethod\n",
    "    def __get_pydantic_core_schema__(cls, source, handler):\n",
    "        \"\"\"\n",
    "        We return a pydantic_core.CoreSchema that behaves in the following ways:\n",
    "\n",
    "        * Data will be cast to ndarrays with the correct dtype\n",
    "        * `ndarrays` instances will be parsed as `ndarrays` and cast to the correct dtype\n",
    "        * Serialization will cast the ndarray to list\n",
    "        \"\"\"\n",
    "        schema = core_schema.no_info_plain_validator_function(cast_to_np)\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906623b",
   "metadata": {},
   "source": [
    "We've added back in our actual \"cast to NumPy\" function we've used previously, and then we have added a function from the `core_schema` object of `pydantic_core`. The `no_info_plain_validator` function which is what generates a schema for `PlainValidator` as we have seen before. We finally return the schema generated from the function, although we can further manipulate it later as needed.\n",
    "\n",
    "There are also other calls such as a [`general_plain_validator_function`](https://docs.pydantic.dev/latest/api/pydantic_core_schema/#pydantic_core.core_schema.general_plain_validator_function) which supports additional info being fed into the function as a secondary argument, or [`no_info_after_validator_function`](https://docs.pydantic.dev/latest/api/pydantic_core_schema/#pydantic_core.core_schema.no_info_after_validator_function) which would make an `AfterValidator`, but we're not going to cover those topics here.\n",
    "\n",
    "Thus far, all we have done is cast to a NumPy array, which is good! We have done that before with the `Annotated` method, but this sets us up for much more powerful manipulation later if we want. We also want to make sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "93489b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "class ArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray\n",
    "\n",
    "print(ArrMolecule(**mol_data).coordinates)\n",
    "print(ArrMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd48e0",
   "metadata": {},
   "source": [
    "Great! We've made a validatable array. Now we're going to extend this approach to handle passing types in as part of the array construction. However, our dtype option isn't used anywhere. To fix that, we're going to expand on this with some of Python's native `typing` tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f6a9b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, TypeVar\n",
    "from pydantic_core import core_schema\n",
    "\n",
    "dtype = TypeVar(\"dtype\")\n",
    "\n",
    "def cast_to_np(v):\n",
    "    try:\n",
    "        v = np.asarray(v)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "    return v\n",
    "\n",
    "\n",
    "class ValidatableArray(Sequence[dtype]):\n",
    "    @classmethod\n",
    "    def __get_pydantic_core_schema__(cls, source, handler):\n",
    "        \"\"\"\n",
    "        We return a pydantic_core.CoreSchema that behaves in the following ways:\n",
    "\n",
    "        * Data will be cast to ndarrays with the correct dtype\n",
    "        * `ndarrays` instances will be parsed as `ndarrays` and cast to the correct dtype\n",
    "        * Serialization will cast the ndarray to list\n",
    "        \"\"\"\n",
    "        schema = core_schema.no_info_plain_validator_function(cast_to_np)\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c9234",
   "metadata": {},
   "source": [
    "We've now established a custom Python type we are calling `dtype`, which is a common term in NumPy space, but we're going to focus on the more general case for now and specialize later.\n",
    "\n",
    "The new object `dtype` is now recognized as a valid Python type, even though nothing in the Python space or any of our modules use this, that's okay! We're going to use it as a placeholder for accepting an index/argument to the `ValidatableArray` class.\n",
    "\n",
    "Speaking of, the `ValidatableArray` is now a subclass of two things: the `Sequence` from Python's `typing` library, and our placeholder `dtype` type as an index/argument. Although it is square brackets, `[]`, we'll refer to these as \"arguments\" as they effectively are for types. We chose the `Sequence` instead of `Generic` from `typing` because at its core, NumPy arrays are sequences, just very formatted and specialized ones. This approach would have worked with `Generic` too, but we're opting to be more verbose.\n",
    "\n",
    "So far, nothing has changed, everything will continue to run exactly as we have designed it previously, however, we can now specify an argument to the `ValidatableArray`. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "437680e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "class ArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray[float]\n",
    "\n",
    "print(ArrMolecule(**mol_data).coordinates)\n",
    "print(ArrMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6fdc4",
   "metadata": {},
   "source": [
    "So now let's change our code to actually do something with that new argument in our function to specify what the dtype should be for the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7010e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, TypeVar\n",
    "from typing_extensions import get_args\n",
    "from pydantic_core import core_schema\n",
    "\n",
    "dtype = TypeVar(\"dtype\")\n",
    "\n",
    "def generate_caster(dtype_input):\n",
    "    def cast_to_np(v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=dtype_input)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "    return cast_to_np\n",
    "\n",
    "\n",
    "class ValidatableArray(Sequence[dtype]):\n",
    "    @classmethod\n",
    "    def __get_pydantic_core_schema__(cls, source, handler):\n",
    "        \"\"\"\n",
    "        We return a pydantic_core.CoreSchema that behaves in the following ways:\n",
    "\n",
    "        * Data will be cast to ndarrays with the correct dtype\n",
    "        * `ndarrays` instances will be parsed as `ndarrays` and cast to the correct dtype\n",
    "        * Serialization will cast the ndarray to list\n",
    "        \"\"\"\n",
    "        dtype_arg = get_args(source)[0]\n",
    "        validator = generate_caster(dtype_arg)\n",
    "        schema = core_schema.no_info_plain_validator_function(validator)\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf729150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n",
      "\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "class FloatArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray[float]\n",
    "\n",
    "class IntArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray[int]\n",
    "\n",
    "print(FloatArrMolecule(**mol_data).coordinates)\n",
    "print(FloatArrMolecule(**mol_data).coordinates)\n",
    "print(\"\")\n",
    "print(IntArrMolecule(**mol_data).coordinates)\n",
    "print(IntArrMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9702df",
   "metadata": {},
   "source": [
    "Ta-da! We've now used the `get_args` function from `typing_extensions` (native in `typing` in Python 3.9+) to get the argument we fed into the `ValidatableArray`, established a generator function for dtypes in `generate_caster`, and then used all of that information to make our NumPy arrays of a specific type. All of this shows the power of the customization we can do with *pydantic*. There are some less-boilerplate ways to do this in *pydantic*, but we leave that up to you to read the docs to find out more. \n",
    "\n",
    "Before we move past this, there are a couple of notes to make about this approach:\n",
    "* This is a relatively slow process in that the generator will be made for every validation, that could be faster.\n",
    "* As written, you MUST pass an arg to `ValidatableArray`, but it could be rewritten to avoid that.\n",
    "\n",
    "We've specifically written this example to use generic Python type objects and methods. However, [NumPy does have its own native types as of 1.20 and 1.21](https://numpy.org/doc/stable/reference/typing.html#numpy.typing.NDArray) we can use instead of the `Generic` and `Sequence`, or defining our own arbitrary type with `TypeVar` to make IDE's happy. Below is the example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74ea709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import get_args, Annotated\n",
    "from pydantic_core import core_schema\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "def generate_caster(dtype):\n",
    "    def cast_to_np(v):\n",
    "        try:\n",
    "            v = np.asarray(v, dtype=dtype)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Could not cast {v} to NumPy Array!\")\n",
    "        return v\n",
    "    return cast_to_np\n",
    "\n",
    "\n",
    "class ValidatableArrayAnnotation:\n",
    "    @classmethod\n",
    "    def __get_pydantic_core_schema__(cls, source, handler):\n",
    "        \"\"\"\n",
    "        We return a pydantic_core.CoreSchema that behaves in the following ways:\n",
    "\n",
    "        * Data will be cast to ndarrays with the correct dtype\n",
    "        * `ndarrays` instances will be parsed as `ndarrays` and cast to the correct dtype\n",
    "        * Serialization will cast the ndarray to list\n",
    "        \"\"\"\n",
    "        shape, dtype_alias = get_args(source)\n",
    "        dtype = get_args(dtype_alias)[0]\n",
    "        validator = generate_caster(dtype)\n",
    "        schema = core_schema.no_info_plain_validator_function(validator)\n",
    "        return schema\n",
    "\n",
    "ValidatableArray = Annotated[NDArray, ValidatableArrayAnnotation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39dda117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]]\n",
      "\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n",
      "[[0 0 0]\n",
      " [1 1 1]\n",
      " [2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "class FloatArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray[float]\n",
    "\n",
    "class IntArrMolecule(Molecule):\n",
    "    coordinates: ValidatableArray[int]\n",
    "\n",
    "print(FloatArrMolecule(**mol_data).coordinates)\n",
    "print(FloatArrMolecule(**mol_data).coordinates)\n",
    "print(\"\")\n",
    "print(IntArrMolecule(**mol_data).coordinates)\n",
    "print(IntArrMolecule(**mol_data).coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75f9bb",
   "metadata": {},
   "source": [
    "This approach now annotates the `NDArray` with additional information as per [the *pydantic* docs](https://docs.pydantic.dev/latest/usage/types/custom/#as-an-annotation) which is then passed to the `ValidatableArrayAnnotation`, and takes use of the NumPy type hint format and behavior for `NDArray`. This *also* has problems as in the end we are trying to reverse engineer a type hint into a formal `dtype` for NumPy, which isn't exactly clear-cut. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8d3ec344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy.ndarray[typing.Any, numpy.dtype[+ScalarType]]\n",
      "numpy.ndarray[typing.Any, numpy.dtype[int]]\n",
      "+ScalarType\n",
      "<class 'typing.TypeVar'>\n"
     ]
    }
   ],
   "source": [
    "print(NDArray)\n",
    "print(NDArray[int])\n",
    "default_second = get_args(get_args(NDArray)[1])[0]\n",
    "print(default_second)\n",
    "print(type(default_second))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1eb7e2",
   "metadata": {},
   "source": [
    "Its very unclear how, if you provide no arguments, to convert the `TypeVar` (which is what you get from the `TypeVar` function of \"+ScalarType\" into `None` which would be the default behavior of `dtype=...` style arguments. Sure you could hard code it, but will that always be the case? That's up to you and beyond what this example hopes to show you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74144c23",
   "metadata": {},
   "source": [
    "### Metaclasses of the Past\n",
    "At one point in this lesson back in Pydantic v1, we talked about [Python Metaclass](https://docs.python.org/3/reference/datamodel.html#metaclasses) as a way to define a class generator whose properties are set dynamically, then usable by the class. BUT...\n",
    "\n",
    "```{admonition} Metaclasses be Forbidden Magics\n",
    ":class: warning\n",
    "“Metaclasses are deeper magic than 99% of users should ever worry about. If you wonder whether you need them, you don’t (the people who actually need them know with certainty that they need them, and don’t need an explanation about why).”\n",
    "\n",
    "— Tim Peters, Author of [Zen of Python, PEP 20](https://peps.python.org/pep-0020/)\n",
    "```\n",
    "\n",
    "Metaclasses are usually not something you want to touch, because you don't need to. The above methods provide a fine way to generate type hints dynamically. However, if you want to be fancy, you can use a Metaclass. The best primer I, Levi Naden, have found on Metaclasses at the time of writing this section (Fall 2022) was through [this Stack Overflow answer](https://stackoverflow.com/a/6581949/10364409).\n",
    "\n",
    "To be honest: you're probably better off writing [a custom core schema as *pydantic* suggests](https://docs.pydantic.dev/latest/usage/types/custom/#customizing-validation-with-__get_pydantic_core_schema__) as above than messing with a Metaclass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac7d83",
   "metadata": {},
   "source": [
    "### Do what makes sense, and only if you need to\n",
    "\n",
    "All of these methods are equally valid, with upsides and downsides alike. Your use case may not even need `dtype` specification and you can just accept the normal NumPy handling of casting to array plus your own custom `validator` functions to make sure the data look correct. Hopefully though this supplemental section has given you ideas to inspire your own code design and give you ideas on interesting and helpful things you can do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
